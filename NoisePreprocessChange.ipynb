{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","history_visible":true,"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!mkdir '/content/backbone/'\n","!mkdir '/content/backbone/resnet'\n","!mkdir '/data'\n","!mkdir '/data/NEW'"],"metadata":{"id":"-1WVZ6a26kTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"mJC0ua8DHh2b"}},{"cell_type":"code","source":["pip install --upgrade tensorboardX"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vHsSQcwLSIH","outputId":"859c27f6-7e00-4774-a0a5-59e45e96d2e5","executionInfo":{"status":"ok","timestamp":1701730440153,"user_tz":300,"elapsed":5478,"user":{"displayName":"jo jo","userId":"04789152650189093642"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}]},{"cell_type":"code","source":["import os\n","import os.path\n","import torch.utils.data as data\n","from PIL import Image\n","import time\n","import datetime\n","import random"],"metadata":{"id":"mq6dqdZWB0tF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","try:\n","    from torch.hub import load_state_dict_from_url\n","except ImportError:\n","    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","\n","\n","from torch.autograd import Variable\n","from torchvision import transforms\n","from collections import OrderedDict\n","from numpy import mean\n","import numpy as np\n","\n","from torch import nn\n","from torch import optim\n","from torch.autograd import Variable\n","from torch.backends import cudnn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from tensorboardX import SummaryWriter\n","from tqdm import tqdm\n","\n"],"metadata":{"id":"I8a3a_dyB6nN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# config"],"metadata":{"id":"B_pE1EDg5Lhg"}},{"cell_type":"code","source":["backbone_path = './backbone/resnet/resnet50-19c8e357.pth'\n","\n","datasets_root = '../data/NEW'\n","\n","cod_training_root = os.path.join(datasets_root, 'Training')\n","\n","chameleon_path = os.path.join(datasets_root, 'test/CHAMELEON')\n","camo_path = os.path.join(datasets_root, 'test/CAMO')\n","cod10k_path = os.path.join(datasets_root, 'test/COD10K')\n","nc4k_path = os.path.join(datasets_root, 'test/NC4K')"],"metadata":{"id":"q2qCtkV95Jyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/TrainDataset.zip' -d '../data/NEW'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"av9Ai_bNgUvS","outputId":"1cdcb1df-8bac-4652-c346-d8b2467167bf","executionInfo":{"status":"ok","timestamp":1701661775588,"user_tz":300,"elapsed":297,"user":{"displayName":"jo jo","userId":"04789152650189093642"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open /content/drive/MyDrive/TrainDataset.zip, /content/drive/MyDrive/TrainDataset.zip.zip or /content/drive/MyDrive/TrainDataset.zip.ZIP.\n"]}]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/TestDataset.zip' -d '../data/NEW'"],"metadata":{"id":"uZo7DDUJ7x-c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Datasets"],"metadata":{"id":"Ao1ZeqZE2ok2"}},{"cell_type":"code","source":["\n","\n","def make_dataset(root):\n","    image_path = os.path.join(root, 'image')\n","    mask_path = os.path.join(root, 'mask')\n","    img_list = [os.path.splitext(f)[0] for f in os.listdir(image_path) if f.endswith('.jpg')]\n","    return [(os.path.join(image_path, img_name + '.jpg'), os.path.join(mask_path, img_name + '.jpg')) for img_name in img_list]\n","\n","class ImageFolder(data.Dataset):\n","    # image and gt should be in the same folder and have same filename except extended name (jpg and png respectively)\n","    def __init__(self, root, joint_transform=None, transform=None, target_transform=None):\n","        self.root = root\n","        self.imgs = make_dataset(root)\n","        self.joint_transform = joint_transform\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __getitem__(self, index):\n","        img_path, gt_path = self.imgs[index]\n","        img = Image.open(img_path).convert('RGB')\n","        target = Image.open(gt_path).convert('L')\n","        if self.joint_transform is not None:\n","            img, target = self.joint_transform(img, target)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n"],"metadata":{"id":"iyHaZKDl2pSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# joint transforms"],"metadata":{"id":"CQ7GmcmK3GYl"}},{"cell_type":"code","source":["\n","\n","#from PIL import Image\n","\n","class Compose(object):\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, mask):\n","        assert img.size == mask.size\n","        for t in self.transforms:\n","            img, mask = t(img, mask)\n","        return img, mask\n","\n","class RandomHorizontallyFlip(object):\n","    def __call__(self, img, mask):\n","        if random.random() < 0.5:\n","            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n","        return img, mask\n","\n","class Resize(object):\n","    def __init__(self, size):\n","        self.size = tuple(reversed(size))  # size: (h, w)  PIL: (w, h)\n","\n","    def __call__(self, img, mask):\n","        assert img.size == mask.size\n","        return img.resize(self.size, Image.BILINEAR), mask.resize(self.size, Image.NEAREST)\n","\n","\n","class AddNoise(object):\n","    def __init__(self, noise_level=80):\n","\n","        self.noise_level = noise_level\n","\n","    def __call__(self, img, mask):\n","        # Convert the PIL image to a NumPy array\n","        np_img = np.array(img)\n","\n","        # Generate Gaussian noise with the specified standard deviation\n","        noise = np.random.normal(0, self.noise_level, np_img.shape)\n","\n","        # Add the noise to the image\n","        np_img = np_img + noise\n","\n","        # Clip the values of the image to ensure they are within 0-255\n","        np_img = np.clip(np_img, 0, 255)\n","\n","        # Convert the NumPy array back to a PIL image and return with the mask\n","        return Image.fromarray(np_img.astype('uint8')), mask\n"],"metadata":{"id":"zAtxTnFg3HPO","executionInfo":{"status":"ok","timestamp":1701897280814,"user_tz":300,"elapsed":21,"user":{"displayName":"_WH shel","userId":"17560401570104777234"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# misc"],"metadata":{"id":"bGo3ykHV4YWw"}},{"cell_type":"code","source":["#import numpy as np\n","#import os\n","\n","class AvgMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def check_mkdir(dir_name):\n","    if not os.path.exists(dir_name):\n","        os.makedirs(dir_name)\n","\n","def _sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"],"metadata":{"id":"13GJJKrF4ZE4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Resnet"],"metadata":{"id":"9eDHzDlNskrt"}},{"cell_type":"code","source":["\n","\n","\n","__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n","           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n","           'wide_resnet50_2', 'wide_resnet101_2']\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n","    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n","    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n","    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n","    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n","}\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","    __constants__ = ['downsample']\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","    __constants__ = ['downsample']\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)\n","\n","\n","def _resnet(arch, block, layers, backbone_path, pretrained, **kwargs):\n","    model = ResNet(block, layers, **kwargs)\n","    if pretrained:\n","        state_dict = torch.load(backbone_path)\n","        model.load_state_dict(state_dict)\n","        print(\"From {} Load {} Weights Succeed!\".format(backbone_path, arch))\n","    return model\n","\n","\n","def resnet18(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-18 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n","                   **kwargs)\n","\n","\n","def resnet34(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-34 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n","                   **kwargs)\n","\n","\n","def resnet50(backbone_path, pretrained=True, **kwargs):\n","    r\"\"\"ResNet-50 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], backbone_path, pretrained, **kwargs)\n","\n","\n","def resnet101(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-101 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n","                   **kwargs)\n","\n","\n","def resnet152(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-152 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n","                   **kwargs)\n","\n","\n","def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNeXt-50 32x4d model from\n","    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    kwargs['groups'] = 32\n","    kwargs['width_per_group'] = 4\n","    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n","                   pretrained, progress, **kwargs)\n","\n","\n","def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNeXt-101 32x8d model from\n","    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    kwargs['groups'] = 32\n","    kwargs['width_per_group'] = 8\n","    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n","                   pretrained, progress, **kwargs)\n","\n","\n","def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"Wide ResNet-50-2 model from\n","    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n","\n","    The model is the same as ResNet except for the bottleneck number of channels\n","    which is twice larger in every block. The number of channels in outer 1x1\n","    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n","    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    kwargs['width_per_group'] = 64 * 2\n","    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n","                   pretrained, progress, **kwargs)\n","\n","\n","def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"Wide ResNet-101-2 model from\n","    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n","\n","    The model is the same as ResNet except for the bottleneck number of channels\n","    which is twice larger in every block. The number of channels in outer 1x1\n","    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n","    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    kwargs['width_per_group'] = 64 * 2\n","    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n","                   pretrained, progress, **kwargs)"],"metadata":{"id":"cgmKRw5BsC4R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PFnet"],"metadata":{"id":"4klL0NVdsltg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4mViHa0FHhOq"},"outputs":[],"source":["\n","\n","###################################################################\n","# ################## Channel Attention Block ######################\n","###################################################################\n","class CA_Block(nn.Module):\n","    def __init__(self, in_dim):\n","        super(CA_Block, self).__init__()\n","        self.chanel_in = in_dim\n","        self.gamma = nn.Parameter(torch.ones(1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps (B X C X H X W)\n","            returns :\n","                out : channel attentive features\n","        \"\"\"\n","        m_batchsize, C, height, width = x.size()\n","        proj_query = x.view(m_batchsize, C, -1)\n","        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n","        energy = torch.bmm(proj_query, proj_key)\n","        attention = self.softmax(energy)\n","        proj_value = x.view(m_batchsize, C, -1)\n","\n","        out = torch.bmm(attention, proj_value)\n","        out = out.view(m_batchsize, C, height, width)\n","\n","        out = self.gamma * out + x\n","        return out\n","\n","###################################################################\n","# ################## Spatial Attention Block ######################\n","###################################################################\n","class SA_Block(nn.Module):\n","    def __init__(self, in_dim):\n","        super(SA_Block, self).__init__()\n","        self.chanel_in = in_dim\n","        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n","        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n","        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.ones(1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps (B X C X H X W)\n","            returns :\n","                out : spatial attentive features\n","        \"\"\"\n","        m_batchsize, C, height, width = x.size()\n","        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n","        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n","        energy = torch.bmm(proj_query, proj_key)\n","        attention = self.softmax(energy)\n","        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n","\n","        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n","        out = out.view(m_batchsize, C, height, width)\n","\n","        out = self.gamma * out + x\n","        return out\n","\n","###################################################################\n","# ################## Context Exploration Block ####################\n","###################################################################\n","class Context_Exploration_Block(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Context_Exploration_Block, self).__init__()\n","        self.input_channels = input_channels\n","        self.channels_single = int(input_channels / 4)\n","\n","        self.p1_channel_reduction = nn.Sequential(\n","            nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p2_channel_reduction = nn.Sequential(\n","            nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p3_channel_reduction = nn.Sequential(\n","            nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p4_channel_reduction = nn.Sequential(\n","            nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","\n","        self.p1 = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, 1, 1, 0),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p1_dc = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=1, dilation=1),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","\n","        self.p2 = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 1),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p2_dc = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=2, dilation=2),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","\n","        self.p3 = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, 5, 1, 2),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p3_dc = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=4, dilation=4),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","\n","        self.p4 = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, 7, 1, 3),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","        self.p4_dc = nn.Sequential(\n","            nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=8, dilation=8),\n","            nn.BatchNorm2d(self.channels_single), nn.ReLU())\n","\n","        self.fusion = nn.Sequential(nn.Conv2d(self.input_channels, self.input_channels, 1, 1, 0),\n","                                    nn.BatchNorm2d(self.input_channels), nn.ReLU())\n","\n","    def forward(self, x):\n","        p1_input = self.p1_channel_reduction(x)\n","        p1 = self.p1(p1_input)\n","        p1_dc = self.p1_dc(p1)\n","\n","        p2_input = self.p2_channel_reduction(x) + p1_dc\n","        p2 = self.p2(p2_input)\n","        p2_dc = self.p2_dc(p2)\n","\n","        p3_input = self.p3_channel_reduction(x) + p2_dc\n","        p3 = self.p3(p3_input)\n","        p3_dc = self.p3_dc(p3)\n","\n","        p4_input = self.p4_channel_reduction(x) + p3_dc\n","        p4 = self.p4(p4_input)\n","        p4_dc = self.p4_dc(p4)\n","\n","        ce = self.fusion(torch.cat((p1_dc, p2_dc, p3_dc, p4_dc), 1))\n","\n","        return ce\n","\n","###################################################################\n","# ##################### Positioning Module ########################\n","###################################################################\n","class Positioning(nn.Module):\n","    def __init__(self, channel):\n","        super(Positioning, self).__init__()\n","        self.channel = channel\n","        self.cab = CA_Block(self.channel)\n","        self.sab = SA_Block(self.channel)\n","        self.map = nn.Conv2d(self.channel, 1, 7, 1, 3)\n","\n","    def forward(self, x):\n","        cab = self.cab(x)\n","        sab = self.sab(cab)\n","        map = self.map(sab)\n","\n","        return sab, map\n","\n","###################################################################\n","# ######################## Focus Module ###########################\n","###################################################################\n","class Focus(nn.Module):\n","    def __init__(self, channel1, channel2):\n","        super(Focus, self).__init__()\n","        self.channel1 = channel1\n","        self.channel2 = channel2\n","\n","        self.up = nn.Sequential(nn.Conv2d(self.channel2, self.channel1, 7, 1, 3),\n","                                nn.BatchNorm2d(self.channel1), nn.ReLU(), nn.UpsamplingBilinear2d(scale_factor=2))\n","\n","        self.input_map = nn.Sequential(nn.UpsamplingBilinear2d(scale_factor=2), nn.Sigmoid())\n","        self.output_map = nn.Conv2d(self.channel1, 1, 7, 1, 3)\n","\n","        self.fp = Context_Exploration_Block(self.channel1)\n","        self.fn = Context_Exploration_Block(self.channel1)\n","        self.alpha = nn.Parameter(torch.ones(1))\n","        self.beta = nn.Parameter(torch.ones(1))\n","        self.bn1 = nn.BatchNorm2d(self.channel1)\n","        self.relu1 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(self.channel1)\n","        self.relu2 = nn.ReLU()\n","\n","    def forward(self, x, y, in_map):\n","        # x; current-level features\n","        # y: higher-level features\n","        # in_map: higher-level prediction\n","\n","        up = self.up(y)\n","\n","        input_map = self.input_map(in_map)\n","        f_feature = x * input_map\n","        b_feature = x * (1 - input_map)\n","\n","        fp = self.fp(f_feature)\n","        fn = self.fn(b_feature)\n","\n","        refine1 = up - (self.alpha * fp)\n","        refine1 = self.bn1(refine1)\n","        refine1 = self.relu1(refine1)\n","\n","        refine2 = refine1 + (self.beta * fn)\n","        refine2 = self.bn2(refine2)\n","        refine2 = self.relu2(refine2)\n","\n","        output_map = self.output_map(refine2)\n","\n","        return refine2, output_map\n","\n","###################################################################\n","# ########################## NETWORK ##############################\n","###################################################################\n","class PFNet(nn.Module):\n","    def __init__(self, backbone_path=None):\n","        super(PFNet, self).__init__()\n","        # params\n","\n","        # backbone\n","        resnetNN = resnet50(backbone_path)\n","        self.layer0 = nn.Sequential(resnetNN.conv1, resnetNN.bn1, resnetNN.relu)\n","        self.layer1 = nn.Sequential(resnetNN.maxpool, resnetNN.layer1)\n","        self.layer2 = resnetNN.layer2\n","        self.layer3 = resnetNN.layer3\n","        self.layer4 = resnetNN.layer4\n","\n","        # channel reduction\n","        self.cr4 = nn.Sequential(nn.Conv2d(2048, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU())\n","        self.cr3 = nn.Sequential(nn.Conv2d(1024, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU())\n","        self.cr2 = nn.Sequential(nn.Conv2d(512, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU())\n","        self.cr1 = nn.Sequential(nn.Conv2d(256, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU())\n","\n","        # positioning\n","        self.positioning = Positioning(512)\n","\n","        # focus\n","        self.focus3 = Focus(256, 512)\n","        self.focus2 = Focus(128, 256)\n","        self.focus1 = Focus(64, 128)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.ReLU):\n","                m.inplace = True\n","\n","    def forward(self, x):\n","        # x: [batch_size, channel=3, h, w]\n","        layer0 = self.layer0(x)  # [-1, 64, h/2, w/2]\n","        layer1 = self.layer1(layer0)  # [-1, 256, h/4, w/4]\n","        layer2 = self.layer2(layer1)  # [-1, 512, h/8, w/8]\n","        layer3 = self.layer3(layer2)  # [-1, 1024, h/16, w/16]\n","        layer4 = self.layer4(layer3)  # [-1, 2048, h/32, w/32]\n","\n","        # channel reduction\n","        cr4 = self.cr4(layer4)\n","        cr3 = self.cr3(layer3)\n","        cr2 = self.cr2(layer2)\n","        cr1 = self.cr1(layer1)\n","\n","        # positioning\n","        positioning, predict4 = self.positioning(cr4)\n","\n","        # focus\n","        focus3, predict3 = self.focus3(cr3, positioning, predict4)\n","        focus2, predict2 = self.focus2(cr2, focus3, predict3)\n","        focus1, predict1 = self.focus1(cr1, focus2, predict2)\n","\n","        # rescale\n","        predict4 = F.interpolate(predict4, size=x.size()[2:], mode='bilinear', align_corners=True)\n","        predict3 = F.interpolate(predict3, size=x.size()[2:], mode='bilinear', align_corners=True)\n","        predict2 = F.interpolate(predict2, size=x.size()[2:], mode='bilinear', align_corners=True)\n","        predict1 = F.interpolate(predict1, size=x.size()[2:], mode='bilinear', align_corners=True)\n","\n","        if self.training:\n","            return predict4, predict3, predict2, predict1\n","\n","        return torch.sigmoid(predict4), torch.sigmoid(predict3), torch.sigmoid(predict2), torch.sigmoid(\n","            predict1)\n"]},{"cell_type":"markdown","source":["# loss"],"metadata":{"id":"aReeM2P94QDp"}},{"cell_type":"code","source":["\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\"\"\"\n","\n","###################################################################\n","# ########################## iou loss #############################\n","###################################################################\n","class IOU(torch.nn.Module):\n","    def __init__(self):\n","        super(IOU, self).__init__()\n","\n","    def _iou(self, pred, target):\n","        pred = torch.sigmoid(pred)\n","        inter = (pred * target).sum(dim=(2, 3))\n","        union = (pred + target).sum(dim=(2, 3)) - inter\n","        iou = 1 - (inter / union)\n","\n","        return iou.mean()\n","\n","    def forward(self, pred, target):\n","        return self._iou(pred, target)\n","\n","###################################################################\n","# #################### structure loss #############################\n","###################################################################\n","class structure_loss(torch.nn.Module):\n","    def __init__(self):\n","        super(structure_loss, self).__init__()\n","\n","    def _structure_loss(self, pred, mask):\n","        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n","        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n","        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n","\n","        pred = torch.sigmoid(pred)\n","        inter = ((pred * mask) * weit).sum(dim=(2, 3))\n","        union = ((pred + mask) * weit).sum(dim=(2, 3))\n","        wiou = 1 - (inter) / (union - inter)\n","        return (wbce + wiou).mean()\n","\n","    def forward(self, pred, mask):\n","        return self._structure_loss(pred, mask)\n"],"metadata":{"id":"e1bNcl344QmJ","executionInfo":{"status":"error","timestamp":1701730428129,"user_tz":300,"elapsed":765,"user":{"displayName":"jo jo","userId":"04789152650189093642"}},"colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"29a63fb1-8e32-4d7f-96da-5f2445483db8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0b67b6e49f5b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ########################## iou loss #############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m###################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mIOU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIOU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"markdown","source":["# train"],"metadata":{"id":"-EiXlUhC4fTo"}},{"cell_type":"code","source":["\"\"\"\n","import datetime\n","import time\n","import os\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.autograd import Variable\n","from torch.backends import cudnn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from tensorboardX import SummaryWriter\n","from tqdm import tqdm\n","\n","import joint_transforms\n","from config import cod_training_root\n","from config import backbone_path\n","from datasets import ImageFolder\n","from misc import AvgMeter, check_mkdir\n","from PFNet import PFNet\n","\n","import loss\n","\"\"\"\n","\n","cudnn.benchmark = True\n","\n","torch.manual_seed(2021)\n","device_ids = [0]\n","\n","ckpt_path = './ckpt'\n","exp_name = 'PFNet'\n","\n","args = {\n","    'epoch_num': 10, #10\n","    'train_batch_size': 16,\n","    'last_epoch': 0,\n","    'lr': 1e-3,\n","    'lr_decay': 0.9,\n","    'weight_decay': 5e-4,\n","    'momentum': 0.9,\n","    'snapshot': '',\n","    'scale': 416,\n","    'save_point': [],\n","    'poly_train': True,\n","    'optimizer': 'SGD',\n","}\n","\n","print(torch.__version__)\n","\n","# Path.\n","check_mkdir(ckpt_path)\n","check_mkdir(os.path.join(ckpt_path, exp_name))\n","vis_path = os.path.join(ckpt_path, exp_name, 'log')\n","check_mkdir(vis_path)\n","log_path = os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt')\n","writer = SummaryWriter(log_dir=vis_path, comment=exp_name)\n","\n","# Transform Data.\n","joint_transform = Compose([\n","    RandomHorizontallyFlip(),\n","    AddNoise()\n","    Resize((args['scale'], args['scale'])),\n","])\n","img_transform = transforms.Compose([\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_transform = transforms.ToTensor()\n","\n","# Prepare Data Set.\n","train_set = ImageFolder(cod_training_root, joint_transform, img_transform, target_transform)\n","print(\"Train set: {}\".format(train_set.__len__()))\n","train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=16, shuffle=True)\n","\n","total_epoch = args['epoch_num'] * len(train_loader)\n","\n","# loss function\n","structure_loss = structure_loss().cuda(device_ids[0])\n","bce_loss = nn.BCEWithLogitsLoss().cuda(device_ids[0])\n","iou_loss = IOU().cuda(device_ids[0])\n","\n","def bce_iou_loss(pred, target):\n","    bce_out = bce_loss(pred, target)\n","    iou_out = iou_loss(pred, target)\n","\n","    loss = bce_out + iou_out\n","\n","    return loss\n","\n","def main():\n","    print(args)\n","    print(exp_name)\n","\n","    net = PFNet(backbone_path).cuda(device_ids[0]).train()\n","\n","    if args['optimizer'] == 'Adam':\n","        print(\"Adam\")\n","        optimizer = optim.Adam([\n","            {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n","             'lr': 2 * args['lr']},\n","            {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n","             'lr': 1 * args['lr'], 'weight_decay': args['weight_decay']}\n","        ])\n","    else:\n","        print(\"SGD\")\n","        optimizer = optim.SGD([\n","            {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n","             'lr': 2 * args['lr']},\n","            {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n","             'lr': 1 * args['lr'], 'weight_decay': args['weight_decay']}\n","        ], momentum=args['momentum'])\n","\n","    if len(args['snapshot']) > 0:\n","        print('Training Resumes From \\'%s\\'' % args['snapshot'])\n","        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'] + '.pth')))\n","        total_epoch = (args['epoch_num'] - int(args['snapshot'])) * len(train_loader)\n","        print(total_epoch)\n","\n","    net = nn.DataParallel(net, device_ids=device_ids)\n","    print(\"Using {} GPU(s) to Train.\".format(len(device_ids)))\n","\n","    open(log_path, 'w').write(str(args) + '\\n\\n')\n","    train(net, optimizer)\n","    writer.close()\n","\n","def train(net, optimizer):\n","    curr_iter = 1\n","    start_time = time.time()\n","\n","    for epoch in range(args['last_epoch'] + 1, args['last_epoch'] + 1 + args['epoch_num']):\n","        loss_record, loss_1_record, loss_2_record, loss_3_record, loss_4_record = AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter()\n","\n","        train_iterator = tqdm(train_loader, total=len(train_loader))\n","        for data in train_iterator:\n","            if args['poly_train']:\n","                base_lr = args['lr'] * (1 - float(curr_iter) / float(total_epoch)) ** args['lr_decay']\n","                optimizer.param_groups[0]['lr'] = 2 * base_lr\n","                optimizer.param_groups[1]['lr'] = 1 * base_lr\n","\n","            inputs, labels = data\n","            batch_size = inputs.size(0)\n","            inputs = Variable(inputs).cuda(device_ids[0])\n","            labels = Variable(labels).cuda(device_ids[0])\n","\n","            optimizer.zero_grad()\n","\n","            predict_1, predict_2, predict_3, predict_4 = net(inputs)\n","\n","            loss_1 = bce_iou_loss(predict_1, labels)\n","            loss_2 = structure_loss(predict_2, labels)\n","            loss_3 = structure_loss(predict_3, labels)\n","            loss_4 = structure_loss(predict_4, labels)\n","\n","            loss = 1 * loss_1 + 1 * loss_2 + 2 * loss_3 + 4 * loss_4\n","\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            loss_record.update(loss.data, batch_size)\n","            loss_1_record.update(loss_1.data, batch_size)\n","            loss_2_record.update(loss_2.data, batch_size)\n","            loss_3_record.update(loss_3.data, batch_size)\n","            loss_4_record.update(loss_4.data, batch_size)\n","\n","            if curr_iter % 10 == 0:\n","                writer.add_scalar('loss', loss, curr_iter)\n","                writer.add_scalar('loss_1', loss_1, curr_iter)\n","                writer.add_scalar('loss_2', loss_2, curr_iter)\n","                writer.add_scalar('loss_3', loss_3, curr_iter)\n","                writer.add_scalar('loss_4', loss_4, curr_iter)\n","\n","            log = '[%3d], [%6d], [%.6f], [%.5f], [%.5f], [%.5f], [%.5f], [%.5f]' % \\\n","                  (epoch, curr_iter, base_lr, loss_record.avg, loss_1_record.avg, loss_2_record.avg,\n","                   loss_3_record.avg, loss_4_record.avg)\n","            train_iterator.set_description(log)\n","            open(log_path, 'a').write(log + '\\n')\n","\n","            curr_iter += 1\n","\n","        if epoch in args['save_point']:\n","            net.cpu()\n","            torch.save(net.module.state_dict(), os.path.join(ckpt_path, exp_name, '%d.pth' % epoch))\n","            net.cuda(device_ids[0])\n","\n","        if epoch >= args['epoch_num']:\n","            net.cpu()\n","            torch.save(net.module.state_dict(), os.path.join(ckpt_path, exp_name, '%d.pth' % epoch))\n","            print(\"Total Training Time: {}\".format(str(datetime.timedelta(seconds=int(time.time() - start_time)))))\n","            print(exp_name)\n","            print(\"Optimization Have Done!\")\n","            return\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"id":"KzHf1K-W4f0Y","outputId":"b4e43f67-00b5-487f-d917-af09d17d98b5","executionInfo":{"status":"error","timestamp":1701724898038,"user_tz":300,"elapsed":176,"user":{"displayName":"_WH shel","userId":"17560401570104777234"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu118\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-6b8a1306b08e>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Prepare Data Set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcod_training_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train set: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-67bb36659e47>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, joint_transform, transform, target_transform)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-67bb36659e47>\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/NEW/Training/image'"]}]},{"cell_type":"markdown","source":["# Infer"],"metadata":{"id":"sLMg5h-i27A3"}},{"cell_type":"code","source":["\"\"\"\n","from config import *\n","from misc import *\n","from PFNet import PFNet\n","\"\"\"\n","\n","torch.manual_seed(2021)\n","device_ids = [0]\n","torch.cuda.set_device(device_ids[0])\n","\n","results_path = './results'\n","check_mkdir(results_path)\n","exp_name = 'PFNetNoise'\n","args = {\n","    'scale': 416,\n","    'save_results': True\n","}\n","\n","print(torch.__version__)\n","\n","img_transform = transforms.Compose([\n","    transforms.Resize((args['scale'], args['scale'])),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","to_pil = transforms.ToPILImage()\n","\n","to_test = OrderedDict([\n","                       ('CHAMELEON', chameleon_path)\n","                       #('CAMO', camo_path),\n","                       #('COD10K', cod10k_path),\n","                       #('NC4K', nc4k_path)\n","                       ])\n","\n","results = OrderedDict()\n","\n","def main():\n","    net = PFNet(backbone_path).cuda(device_ids[0])\n","    net.load_state_dict(torch.load('PFNet.pth'))\n","    print('Load {} succeed!'.format('PFNet.pth'))\n","\n","    net.eval()\n","    with torch.no_grad():\n","        start = time.time()\n","        for name, root in to_test.items():\n","            time_list = []\n","            image_path = os.path.join(root, 'image')\n","\n","            if args['save_results']:\n","                check_mkdir(os.path.join(results_path, exp_name, name))\n","\n","            img_list = [os.path.splitext(f)[0] for f in os.listdir(image_path) if f.endswith('jpg')]\n","            mask_path = os.path.join(root, 'mask')\n","            scoreAverage = 0\n","            dataNum = len(img_list)\n","            for idx, img_name in enumerate(img_list):\n","                img = Image.open(os.path.join(image_path, img_name + '.jpg')).convert('RGB')\n","\n","                w, h = img.size\n","                img_var = Variable(img_transform(img).unsqueeze(0)).cuda(device_ids[0])\n","\n","                start_each = time.time()\n","                _, _, _, prediction = net(img_var)\n","                time_each = time.time() - start_each\n","                time_list.append(time_each)\n","\n","                prediction = np.array(transforms.Resize((h, w))(to_pil(prediction.data.squeeze(0).cpu())))\n","\n","                #E-Measure Accuracy\n","                mask = Image.open(os.path.join(mask_path, img_name + '.png')).convert('L')\n","                pix = np.asarray(mask)\n","                #if statement to check if any mask and image aren't the same size\n","                if(prediction.shape == pix.shape):\n","                    predMean = np.mean(prediction)\n","                    pixMean = np.mean(pix)\n","\n","                    #Alignment\n","                    predAlign =  prediction - predMean\n","                    pixAlign = pix - pixMean\n","                    eps = np.finfo(np.float64).eps\n","                    align = 2*(predAlign * pixAlign)/(predAlign * predAlign + pixAlign * pixAlign + eps)\n","\n","                    #Enhance Alignment\n","                    enhanceAlign = np.power((align+1),2)/4\n","\n","                    #Calculate score and add to total\n","                    score = np.sum(enhanceAlign)/(w*h - 1 + eps)\n","                    scoreAverage += score\n","                  #Find data which is not correctly shaped and subtract from dataNum\n","                else:\n","                    print(prediction.shape)\n","                    print(pix.shape)\n","                    print(img_name)\n","                    dataNum -= 1\n","\n","                if args['save_results']:\n","                    Image.fromarray(prediction).convert('L').save(os.path.join(results_path, exp_name, name, img_name + '.png'))\n","            print(('{}'.format(exp_name)))\n","            print(\"{}'s average Time Is : {:.3f} s\".format(name, mean(time_list)))\n","            print(\"{}'s average Time Is : {:.1f} fps\".format(name, 1 / mean(time_list)))\n","            if dataNum != 0:\n","              print(\"E-measure accuracy: {}\".format(scoreAverage/dataNum))\n","            else:\n","              print(\"Error: dataNum is zero\")\n","\n","\n","    end = time.time()\n","    print(\"Total Testing Time: {}\".format(str(datetime.timedelta(seconds=int(end - start)))))\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"462J2pIo27_G","colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"202d49c8-1257-4131-d3c0-08e6fcfa1f6b","executionInfo":{"status":"error","timestamp":1701661249385,"user_tz":300,"elapsed":218,"user":{"displayName":"_WH shel","userId":"17560401570104777234"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cf67e4d37a7c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdevice_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]}]}